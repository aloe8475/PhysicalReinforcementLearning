{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries \n",
    "import os\n",
    "import csv\n",
    "import pandas \n",
    "import numpy as np\n",
    "from tkinter import*\n",
    "from tkinter import filedialog\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show, push_notebook\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.layouts import column,row,gridplot,layout\n",
    "from bokeh.models.widgets import Panel,Tabs\n",
    "from bokeh.models import LinearAxis,Range1d\n",
    "from IPython.core.debugger import set_trace\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from bokeh.palettes import all_palettes\n",
    "output_notebook()\n",
    "\n",
    "\n",
    "\n",
    "global default_folder\n",
    "default_folder='/import/silo2/aloe8475/Documents/Data/Associative Learning/EquilProp/Adrian Results/selected_datasamples/'\n",
    "\n",
    "global downsample_rate\n",
    "downsample_rate=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#opening files and transform to pandas function definitions\n",
    "\n",
    "def open_dialog():\n",
    "    # Create Tk root\n",
    "    root = Tk()\n",
    "    # Hide the main window\n",
    "    root.withdraw()\n",
    "    root.call('wm', 'attributes', '.', '-topmost', True)\n",
    "\n",
    "  \n",
    "    infiles = filedialog.askopenfilename(multiple=True)\n",
    "    print('Number of files:','  ',len(infiles))\n",
    "    return infiles\n",
    "\n",
    "def HeaderReader(csVReader):\n",
    "    #read csv file header\n",
    "    headlen=4\n",
    "    lin=[]\n",
    "    for i in range(headlen-1):\n",
    "        line=next(csVReader)\n",
    "        lin.append(line)\n",
    "    return dict(zip(lin[0],lin[2]))\n",
    "\n",
    "def ChannelReader(channelstring):\n",
    "    #prepare metadata\n",
    "    columns=channelstring['channel_guide'].split(sep=',')\n",
    "    chans=list()\n",
    "    for col in columns:\n",
    "        if col.find('/a')!=(-1):\n",
    "            col=col[(col.find('/a')+1):]\n",
    "        chans.append(col)\n",
    "    return chans\n",
    "def process_file(pandata,header):\n",
    "    \"\"\"\n",
    "    processing files of n_back according to current file formatting\n",
    "    There is a marker between intervals (number 999). The next two rows of data\n",
    "    contain information about which pattern was in input(4 electrode)/output(2 electrode) \n",
    "    and whether it was train or test interval.\n",
    "    Example: \n",
    "        out1 out2\n",
    "    idx\n",
    "   100     999  999  -> 999 is marker of previous interval end\n",
    "   101     5    0    -> 5 is input pattern (0101 in binary, ie. two electrodes opened). 0 is train interval (1 would be test)\n",
    "   102     1    -1   -> 1 is output pattern (01 in binary, ie. one output electrode being trained). -1 does not have meaning.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    sel=list(pandata['out1'][pandata['out1']==999].index)\n",
    "    sel2=sel.copy()\n",
    "    sel.insert(0,-3)\n",
    "    sel.pop(-1)\n",
    "    \n",
    "    index_start=list()\n",
    "    index_end=list()\n",
    "    interval=list()\n",
    "    pat=list()\n",
    "    outpat=list()\n",
    "    descript=list()\n",
    "    Vout1=list()\n",
    "    Vout2=list()\n",
    "\n",
    "    TrainOrTest=lambda a:'Train' if a==0 else 'Test' #function that returns 'Train' if input is 0, otherwise 'Test'\n",
    "    \n",
    "    bin_str= lambda a: bin(int(a))[2:] if bin(int(a))[2:][-1]=='0' else '0'+bin(int(a))[2:] #function that converts inputs into binary\n",
    "    \n",
    "    for i1,i2 in zip(sel,sel2):\n",
    "        index_start.append(i1+3)\n",
    "        index_end.append(i2)\n",
    "        interval.append(TrainOrTest(pandata.iloc[i2+1,1]))\n",
    "        pat.append(pandata.iloc[i2+1,0])\n",
    "        outpat.append(pandata.iloc[i2+2,0])\n",
    "        descript.append(TrainOrTest(pandata.iloc[i2+1,1])+'_IN:'+bin_str(pat[-1])+'__OUT:'+bin_str(outpat[-1]))\n",
    "        vo1=np.array(pandata['out1'][i1+3:i2])\n",
    "        extra_v1=np.random.choice(vo1[-10:],3) #this is just for index matching w. time. does not impact results.\n",
    "        vo2=np.array(pandata['out2'][i1+3:i2])\n",
    "        extra_v2=np.random.choice(vo2[-10:],3)\n",
    "        Vout1.extend(list(np.concatenate((vo1,extra_v1))))\n",
    "        Vout2.extend(list(np.concatenate((vo2,extra_v2))))\n",
    "        \n",
    "    index_start=pandas.DataFrame(index_start)\n",
    "    index_end=pandas.DataFrame(index_end)\n",
    "    interval=pandas.DataFrame(interval)\n",
    "    pat=pandas.DataFrame(pat)\n",
    "    outpat=pandas.DataFrame(outpat)\n",
    "    descript=pandas.DataFrame(descript) #an array of strings that tell us the input pattern and the output pattern, as well as if iteration was Train or Test\n",
    "    \n",
    "    Vout1=pandas.DataFrame(Vout1)\n",
    "    Vout2=pandas.DataFrame(Vout2)\n",
    "    Time=pandas.DataFrame(CreateTimeArray(pandata['out1'],header,downsample=downsample_rate))\n",
    "   \n",
    "    pdout=pandas.concat([pandata,Time,Vout1,Vout2,index_start,index_end,interval,pat,outpat,descript]\n",
    "                     ,axis=1,ignore_index=True)\n",
    "    \n",
    "    pdout.columns=['out1','out2','Time','Vout1','Vout2','index_start','index_end','interval','pat',\n",
    "                  'outpat','descript']\n",
    "    return pdout\n",
    "    \n",
    "def CreateTimeArray(pandata,header=[],downsample=1,rate=1):\n",
    "    #create time array from sampled acquisition using downsampling\n",
    "    tlen=pandata.shape[0]\n",
    "    if header:\n",
    "       \n",
    "        rate=np.divide(1,int(header['Rate'])/downsample)\n",
    "        \n",
    "    else:\n",
    "        rate=np.divide(1,int(rate)/downsample)\n",
    "    \n",
    "    return np.linspace(0,rate*tlen,tlen)\n",
    "def open_and_transform(infiles=[]):\n",
    "    #file dialog for chosing files and transform data to pandas\n",
    "    datafiles=list()\n",
    "    metafiles=list()\n",
    "    filenames=list()\n",
    "    \n",
    "    if not infiles:\n",
    "        infiles=open_dialog()\n",
    "    \n",
    "    for fpath in infiles:\n",
    "        fpath=default_folder+fpath #alon added to find the data folder\n",
    "        with open(fpath) as csvDataFile:\n",
    "            csvReader=csv.reader(csvDataFile)\n",
    "            header=HeaderReader(csvReader)\n",
    "\n",
    "        #chan_names=ChannelReader(header)\n",
    "        chan_names=['out1','out2']\n",
    "        df=pandas.read_csv(fpath,delimiter=';',skiprows=4,names=chan_names)\n",
    "        index0=df['out1'][df['out1']==0].index[0:]\n",
    "        df=df.drop(index0)\n",
    "        df['out1']\n",
    "        #print('channels:',chan_names)\n",
    "        '''\n",
    "        Create time array on timedata series. \n",
    "        '''\n",
    "        #ndf=CreateTimeArray(header,df)\n",
    "        #ndf.columns\n",
    "        datafiles.append(df)\n",
    "        metafiles.append(header)\n",
    "        filenames.append(fpath[fpath.rfind('/')+1:])\n",
    "        print(fpath)\n",
    "    return datafiles,metafiles,filenames\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_multiple(pandata,header,indexes=0,pattern='all',interval='both'):\n",
    "    cPat=lambda pat: pandata['pat'].dropna().unique() if pat=='all' else [pat] #function that returns unique patterns\n",
    "    cInt=lambda inter: ['Train','Test'] if inter=='both' else [inter] #function that returns ['Train','Test'] if input 'both'\n",
    "    if (indexes==0):\n",
    "        slice_ind=pandata.loc[:,['index_start','index_end']][(pandata['pat'].isin(cPat(pattern))) & (pandata['interval'].isin(cInt(interval)))]\n",
    "    else:\n",
    "        pandataS=pandata.loc[indexes]\n",
    "        slice_ind=pandataS.loc[:,['index_start','index_end']][(pandataS['pat'].isin(cPat(pattern))) & (pandataS['interval'].isin(cInt(interval)))]\n",
    "   \n",
    "\n",
    "    fg=figure(title='Current(V) vs. Time(s)',x_axis_label='Time(s)',y_axis_label='Current(V)')\n",
    "    fg.xgrid.grid_line_color='gray'\n",
    "    fg.ygrid.grid_line_color='gray'\n",
    "    fg.xgrid.grid_line_alpha=0.5\n",
    "    fg.ygrid.grid_line_alpha=0.5\n",
    "    fg.outline_line_alpha=0.8\n",
    "    fg.outline_line_color='black'\n",
    "    \n",
    "    \n",
    "    for idx,row in slice_ind.iterrows(): #iterate through each time start and time stop (row)\n",
    "            i1=row['index_start']\n",
    "            i2=row['index_end']\n",
    "            y1=pandata.loc[i1:i2-1,'out1']\n",
    "            y2=pandata.loc[i1:i2-1,'out2']\n",
    "            x=CreateTimeArray(y1,header,downsample_rate)\n",
    "            fg.line(x,y1,color='blue',legend_label='Chanel1')\n",
    "            fg.line(x,y2,color='red',legend_label='Chanel2')\n",
    "    return fg\n",
    "\n",
    "def plot_special(pandata,header):\n",
    "    sliceT=pandata.loc[:,['index_start','index_end']][pandata['interval']=='Test']\n",
    "    target=pandata.loc[:,['outpat']][pandata['interval']=='Test']\n",
    "    \n",
    "    checkAcc=lambda result,target: 1 if result==target else 0\n",
    "    \n",
    "    #plot whole time series\n",
    "   \n",
    "    y1=pandata['Vout1']\n",
    "    y2=pandata['Vout2']\n",
    "    tot_max=max(max(y1),max(y2))\n",
    "    tot_min=min(min(y1),min(y2))\n",
    "    y_range=Range1d(start=-0.01,end=tot_max+0.1*tot_max)\n",
    "    fg=figure(title='Current(v) Time(s) w. accuracy',x_axis_label='Time(s)',y_axis_label='Current(V)',y_range=y_range)\n",
    "    t=CreateTimeArray(y1,header,downsample_rate)\n",
    "    fg.line(t,y1,color='blue',legend_label='Chanel1')\n",
    "    fg.line(t,y2,color='red',legend_label='Chanel2')\n",
    "    \n",
    "    \n",
    "    #plot scatter dots accuracy\n",
    "    x_ac1=list()\n",
    "    y_ac1=list()\n",
    "    \n",
    "    x_ac2=list()\n",
    "    y_ac2=list()\n",
    "    for idx,row in sliceT.iterrows():\n",
    "        testing_pat=pandata.loc[idx,'pat']\n",
    "\n",
    "        i1=row['index_start']\n",
    "        i2=row['index_end']\n",
    "        y1=pandata.loc[i1:i2-1,'out1']\n",
    "        y2=pandata.loc[i1:i2-1,'out2']\n",
    "\n",
    "        av=[y1.mean(),y2.mean()]\n",
    "        booled=[int(el/max(av)) for el in av]\n",
    "        tested_pat=bool_to_int(booled)\n",
    "\n",
    "        target=int(pandata['outpat'][(pandata['interval']=='Train') & (pandata['pat']==testing_pat)].iloc[0])\n",
    "        #print(av)\n",
    "        #print(booled)\n",
    "        #print(tested_pat)\n",
    "        #print(target)\n",
    "        if target==1:\n",
    "            x_ac1.append(pandata.loc[i1,'Time'])\n",
    "            y_ac1.append(checkAcc(tested_pat,target))\n",
    "        elif target==2:\n",
    "            x_ac2.append(pandata.loc[i1,'Time'])\n",
    "            y_ac2.append(checkAcc(tested_pat,target))\n",
    "            \n",
    "    comb=y_ac1+y_ac2\n",
    "    perc=sum(comb)/len(comb)\n",
    "    print('Percentage of correct :')\n",
    "    print('-----------------------')\n",
    "    print(perc)\n",
    "    print('.......................')\n",
    "    fg.extra_y_ranges={'acc':Range1d(start=-0.01,end=1.2)}\n",
    "    fg.add_layout(LinearAxis(y_range_name='acc',axis_label='Accuracy'),'right')\n",
    "    fg.scatter(x_ac1,y_ac1,color='blue',y_range_name='acc',size=20)\n",
    "    fg.scatter(x_ac2,y_ac2,color='red',y_range_name='acc',size=20)\n",
    "    return fg\n",
    "    \n",
    "def select_and_plot(datafiles,name='net1'):\n",
    "    #--------------------------------------\n",
    "    selector=list(range(len(datafiles))) #select all files by default\n",
    "\n",
    "    sel_dat=[datafiles[i] for i in selector]\n",
    "\n",
    "    chan_names=[list(sel_dat[i]) for i in selector]\n",
    "\n",
    "    #--------------------------------------\n",
    "    li_dat=average_phase(sel_dat,chan_names)\n",
    "    plot=create_bokeh_li_layout(li_dat,name=name)\n",
    "    show(plot)\n",
    "    return \n",
    "\n",
    "def bool_to_int(bool_arr):\n",
    "    numb=0\n",
    "\n",
    "    for idx,el in enumerate(bool_arr):\n",
    "        numb=2**(idx)*el+numb\n",
    "    return numb\n",
    "def int_to_bool_array(int_pat):\n",
    "        pat = [int(d) for d in str(bin(int(int_pat)))[2:]]\n",
    "        pat.reverse()\n",
    "        if len(pat)<9:\n",
    "            pad=[0 for i in range(9-len(pat))]\n",
    "            pat.extend(pad)\n",
    "        # print(pat)\n",
    "        return np.array(pat)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames=os.listdir(default_folder)\n",
    "fnames=[fn  for fn in fnames if fn.endswith('.csv')]\n",
    "print(fnames)\n",
    "\n",
    "\n",
    "# you can select here few files if you dont want to take them all\n",
    "fnames=fnames[1:3]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#window prompt\n",
    "# datafiles,metafiles,filenames=open_and_transform()\n",
    "\n",
    "\n",
    "#manual file selection\n",
    "datafiles,metafiles,filenames=open_and_transform(infiles=fnames)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print('Header')\n",
    "\n",
    "#print('Order of measure')\n",
    "#print('----------------')\n",
    "#print(df['descript'].dropna())\n",
    "#print('---------------')\n",
    "#print(df.head())\n",
    "\n",
    "# Patterns trained in this sample\n",
    "downsample_rate=10\n",
    "#print(df['pat'].dropna().unique())\n",
    "\n",
    "i=0\n",
    "df=[None]*len(datafiles)\n",
    "for raw_file,meta,name in zip(datafiles,metafiles,filenames):\n",
    "    print('\\n\\n\\n\\n')\n",
    "    print('FileName:')\n",
    "    print('---------')\n",
    "    print(name)\n",
    "\n",
    "    print('---------')\n",
    "    print('Patterns: ',meta['PatList'])\n",
    "    print('TrainVolt: ',meta['TrainVolt'])\n",
    "    print('TestVolt:  ',meta['TestVolt'])\n",
    "    print('GradSetpoint',meta['GradSetpoint'])\n",
    "    print('VControlMax',meta['VControlMax'])\n",
    "    print('PatThreshold',meta['PatThresh'])\n",
    "    print('---------')\n",
    "    print('---------')\n",
    "\n",
    "    \n",
    "    df[i]=process_file(raw_file,meta)\n",
    "    \n",
    "    #figure 1. spliting curves w. different conditions. \n",
    "   \n",
    "    fig1=plot_multiple(df[i],meta,indexes=0,interval='Train')\n",
    "    #fig1=plot_multiple(df,meta,indexes=0,interval='Test') #plot testing curves\n",
    "    fig1.plot_width=950\n",
    "    fig1.plot_height=650\n",
    "    show(fig1)\n",
    "    \n",
    "    l1=[*range(5,10,1)] #you can select different indexes changing the range\n",
    "    fig2=plot_multiple(df[i],meta,indexes=l1,interval='Train')\n",
    "    fig2.plot_width=950\n",
    "    fig2.plot_height=650\n",
    "    show(fig2)\n",
    "    \n",
    "\n",
    "    #figure 2. showing whole n-back w. 2 output electrodes results and accuracy. \n",
    "\n",
    "\n",
    "    fig3=plot_special(df[i],meta)\n",
    "    fig3.plot_width=950\n",
    "    fig3.plot_height=650\n",
    "    show(fig3)\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
